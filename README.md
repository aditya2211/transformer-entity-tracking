
# Effective Use of Transformer Networks for Entity Tracking (EMNLP19)

This is a PyTorch implementation of our [EMNLP paper]() on the effectiveness of pre-trained transformer architextures in capturing complex entity interaction in procedural texts. 

## Dependencies 

The code was developed by extending [Hugging Face's implementations of OpenAI's GPT](https://github.com/huggingface/pytorch-openai-transformer-lm) and [BERT](https://github.com/huggingface/transformers).

## Dataset and code
The dataset for two tasks: (i) Recipes, and (ii) ProPara can be found [here](https://drive.google.com/file/d/1Y9DUPSiabnBhSoPLLgmGsVE_Gf4if1az/view) in the appropriate directories.

The codebase consists of two main dorectories:
### `gpt-entity-tracking`
 


## Citation
```
 @inproceedings{gupta-durrett-2019-entity-tracking,
    title = "Effective Use of Transformer Networks for Entity Tracking",
    author = "Gupta, Aditya  and
      Durrett, Greg",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
}
```
